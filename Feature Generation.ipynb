{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import sklearn as sk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from hmmlearn import hmm , vhmm\n",
    "import scipy.fftpack as fftpack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from ta.trend import sma_indicator\n",
    "from ta.momentum import rsi\n",
    "from ta.momentum import stoch\n",
    "from ta.trend import ema_indicator\n",
    "from ta.trend import adx\n",
    "from ta.trend import macd\n",
    "from ta.volume import on_balance_volume\n",
    "import random\n",
    "import itertools\n",
    "from collections import deque\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad3adc2",
   "metadata": {},
   "source": [
    "# Generate Features and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class data_processor:\n",
    "    def __init__(self,tickers,start_date='1980-01-01',end_date='2025-04-30', window_sizes=[24,24*3]):\n",
    "        self.ticker = tickers\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.window_sizes = window_sizes\n",
    "\n",
    "\n",
    "        self.data= None\n",
    "\n",
    "    def get_data(self):\n",
    "        df1 = yf.download(self.ticker[0],interval='1h',start=self.start_date, end=self.end_date)\n",
    "        df2 = yf.download(self.ticker[1],interval='1h',start=self.start_date, end=self.end_date)\n",
    "       \n",
    "        # df1 = yf.download(self.ticker[0],start=self.start_date, end=self.end_date)\n",
    "        # df2 = yf.download(self.ticker[1],start=self.start_date, end=self.end_date)\n",
    "        df1.columns = df1.columns.droplevel(1)\n",
    "        df2.columns = df2.columns.droplevel(1)\n",
    "     \n",
    "\n",
    "\n",
    "        df1['Return1'] = np.log(df1['Close']) - np.log(df1['Close']).shift(1)\n",
    "        df2['Return2'] = np.log(df2['Close']) - np.log(df2['Close']).shift(1)\n",
    "\n",
    "\n",
    "        df1['rsi'] = rsi(df1[( 'Close')], 14)   \n",
    "        df1['macd'] = macd(df1[( 'Close')])\n",
    "        df1['obv'] = on_balance_volume(df1[( 'Close')], df1[( 'Volume')])\n",
    "\n",
    "        df2['rsi'] = rsi(df2[( 'Close')], 14) \n",
    "        df2['macd'] = macd(df2[( 'Close')])\n",
    "        df2['obv'] = on_balance_volume(df2[( 'Close')], df2[( 'Volume')])\n",
    "        \n",
    "        df1 , df2 = df1.align(df2,join='inner',axis=0)\n",
    "\n",
    "    \n",
    "        self.df1 = df1\n",
    "        self.df2 = df2\n",
    "    \n",
    "    def get_hedge_ratio(self,window_size,clopen = 'Close'):\n",
    "        pairs_stats = [] # date, close1, close2, hedge ratio, half life, spread (z-score), adf p-value\n",
    "\n",
    "        for i in range(len(self.df1)-window_size):\n",
    "            window1 = self.df1[clopen][i:i+window_size]\n",
    "            window2 = self.df2[clopen][i:i+window_size]\n",
    "            last_index = window1.index[-1]\n",
    "\n",
    "            model = sm.OLS(window1,window2).fit()\n",
    "            hedge_ratio = model.params[0]\n",
    "            spread = window2 - hedge_ratio*window1 \n",
    "            spread_z_score = (spread - spread.mean())/spread.std()\n",
    "            half_life = np.log(2)/hedge_ratio\n",
    "\n",
    "            adf = adfuller(spread)[1]\n",
    "\n",
    "            pairs_stats.append([last_index,self.df1.loc[last_index, 'Return1'],self.df2.loc[last_index, 'Return2'],self.df2.loc[last_index, 'rsi'],self.df2.loc[last_index, 'macd'],self.df2.loc[last_index, 'obv'],self.df1.loc[last_index, 'rsi'],self.df1.loc[last_index, 'macd'],self.df1.loc[last_index, 'obv']                  ,hedge_ratio,half_life,spread_z_score.iloc[-1],adf])\n",
    "         \n",
    "\n",
    "        data = pd.DataFrame(data= pairs_stats,columns=['Date','Return1','Return2','rsi2','macd2','obv2','rsi1','macd1','obv1','Hedge Ratio','Half Life','Spread','ADF'])\n",
    "        data = data.set_index('Date')\n",
    "        return data\n",
    "\n",
    "    def windows(self):\n",
    "        data1 = self.get_hedge_ratio(self.window_sizes[0])\n",
    "        data2 = self.get_hedge_ratio(self.window_sizes[1])\n",
    "\n",
    "        data = data1.join(data2,rsuffix=' Long')\n",
    "        data = data.dropna()\n",
    "        data = data.drop(['Return1 Long','Return2 Long','rsi2 Long','macd2 Long','obv2 Long','rsi1 Long','macd1 Long','obv1 Long'],axis=1)\n",
    "      \n",
    "\n",
    "        self.data=data\n",
    "    def clopen(self):\n",
    "        data1 = self.get_hedge_ratio(252,'Adj Close')\n",
    "        data2 = self.get_hedge_ratio(252,'Open')\n",
    "\n",
    "        data = data1.join(data2,rsuffix=' Open')\n",
    "        data = data.dropna()\n",
    "        data = data.drop(['Return1 Open','Return2 Open','rsi2 Open','macd2 Open','obv2 Open','rsi1 Open','macd1 Open','obv1 Open'],axis=1)\n",
    "\n",
    "\n",
    "        self.data=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ca2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = data_processor(['DG','DLTR'],start_date='2023-05-27',end_date='2025-05-25')\n",
    "processor.get_data()\n",
    "processor.windows()\n",
    "print(processor.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4efe84",
   "metadata": {},
   "source": [
    "# Find Cointegrated Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc58521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def download_stock_data(tickers, start_date, end_date):\n",
    "    \n",
    "    stock_data = {}\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            data = yf.download(ticker, interval='1h', start=start_date, end=end_date)\n",
    "            if not data.empty:\n",
    "                # Use adjusted close price\n",
    "                data.columns.droplevel(1)\n",
    "                stock_data[ticker] = data['Close']\n",
    "\n",
    "                print(f\"Downloaded data for {ticker}: {len(data)} records\")\n",
    "            else:\n",
    "                print(f\"No data found for {ticker}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {ticker}: {e}\")\n",
    "    \n",
    "    return stock_data\n",
    "\n",
    "def perform_johansen_test(price_series1, price_series2, det_order=0, k_ar_diff=1):\n",
    "    \n",
    "    # Align the series by index and remove NaN values\n",
    "    combined_data = pd.concat([price_series1, price_series2], axis=1).dropna()\n",
    "    \n",
    "    if len(combined_data) < 20:  # Need sufficient data points\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Perform Johansen test\n",
    "        result = coint_johansen(combined_data.values, det_order=det_order, k_ar_diff=k_ar_diff)\n",
    "        \n",
    "        # Extract test statistics and critical values\n",
    "        trace_stat = result.lr1[0]  # Trace statistic for r=0 (no cointegration)\n",
    "        max_eigen_stat = result.lr2[0]  # Max eigenvalue statistic for r=0\n",
    "        \n",
    "        # Critical values for trace test (90%, 95%, 99%)\n",
    "        trace_crit_values = result.cvt[0]  # [90%, 95%, 99%]\n",
    "        \n",
    "        # Critical values for max eigenvalue test (90%, 95%, 99%)\n",
    "        max_eigen_crit_values = result.cvm[0]  # [90%, 95%, 99%]\n",
    "        \n",
    "        # Determine significance levels\n",
    "        trace_significance = []\n",
    "        if trace_stat > trace_crit_values[2]:  # 99%\n",
    "            trace_significance.append('99%')\n",
    "        if trace_stat > trace_crit_values[1]:  # 95%\n",
    "            trace_significance.append('95%')\n",
    "        if trace_stat > trace_crit_values[0]:  # 90%\n",
    "            trace_significance.append('90%')\n",
    "        \n",
    "        max_eigen_significance = []\n",
    "        if max_eigen_stat > max_eigen_crit_values[2]:  # 99%\n",
    "            max_eigen_significance.append('99%')\n",
    "        if max_eigen_stat > max_eigen_crit_values[1]:  # 95%\n",
    "            max_eigen_significance.append('95%')\n",
    "        if max_eigen_stat > max_eigen_crit_values[0]:  # 90%\n",
    "            max_eigen_significance.append('90%')\n",
    "        \n",
    "        return {\n",
    "            'trace_statistic': trace_stat,\n",
    "            'max_eigen_statistic': max_eigen_stat,\n",
    "            'trace_critical_values': {\n",
    "                '90%': trace_crit_values[0],\n",
    "                '95%': trace_crit_values[1],\n",
    "                '99%': trace_crit_values[2]\n",
    "            },\n",
    "            'max_eigen_critical_values': {\n",
    "                '90%': max_eigen_crit_values[0],\n",
    "                '95%': max_eigen_crit_values[1],\n",
    "                '99%': max_eigen_crit_values[2]\n",
    "            },\n",
    "            'trace_significance': trace_significance,\n",
    "            'max_eigen_significance': max_eigen_significance,\n",
    "            'data_points': len(combined_data)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Johansen test: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_cointegration(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Main function to analyze cointegration between all pairs of tickers\n",
    "    \"\"\"\n",
    "    # Download data\n",
    "    print(\"Downloading stock data...\")\n",
    "    stock_data = download_stock_data(tickers, start_date, end_date)\n",
    "    \n",
    "    if len(stock_data) < 2:\n",
    "        print(\"Need at least 2 valid tickers for cointegration analysis\")\n",
    "        return\n",
    "    \n",
    "    # Generate all pairs\n",
    "    valid_tickers = list(stock_data.keys())\n",
    "    pairs = list(combinations(valid_tickers, 2))\n",
    "    \n",
    "    print(f\"\\nTesting {len(pairs)} pairs for cointegration...\")\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    for ticker1, ticker2 in pairs:\n",
    "        print(f\"Testing {ticker1} - {ticker2}\")\n",
    "        \n",
    "        result = perform_johansen_test(stock_data[ticker1], stock_data[ticker2])\n",
    "        \n",
    "        if result:\n",
    "            results.append({\n",
    "                'Pair': f\"{ticker1} - {ticker2}\",\n",
    "                'Trace_Statistic': result['trace_statistic'],\n",
    "                'Max_Eigen_Statistic': result['max_eigen_statistic'],\n",
    "                'Trace_90%_Critical': result['trace_critical_values']['90%'],\n",
    "                'Trace_95%_Critical': result['trace_critical_values']['95%'],\n",
    "                'Trace_99%_Critical': result['trace_critical_values']['99%'],\n",
    "                'Max_Eigen_90%_Critical': result['max_eigen_critical_values']['90%'],\n",
    "                'Max_Eigen_95%_Critical': result['max_eigen_critical_values']['95%'],\n",
    "                'Max_Eigen_99%_Critical': result['max_eigen_critical_values']['99%'],\n",
    "                'Trace_Significance': ', '.join(result['trace_significance']) if result['trace_significance'] else 'Not Significant',\n",
    "                'Max_Eigen_Significance': ', '.join(result['max_eigen_significance']) if result['max_eigen_significance'] else 'Not Significant',\n",
    "                'Data_Points': result['data_points']\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame and sort by trace statistic (descending)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results = df_results.sort_values('Trace_Statistic', ascending=False)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['DG','DLTR',\"CPT\",\"EQR\"]\n",
    "# payload=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "# first_table = payload[0]\n",
    "# second_table = payload[1]\n",
    "\n",
    "# df = first_table\n",
    "# tickers = df.Symbol.tolist()[:250]\n",
    "\n",
    "# Define date range\n",
    "end_date = '2025-05-25'\n",
    "start_date = '2023-05-27'\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Analyzing cointegration from {start_date} to {end_date}\")\n",
    "\n",
    "# Perform analysis\n",
    "results_df = analyze_cointegration(tickers, start_date, end_date)\n",
    "\n",
    "if results_df is not None and not results_df.empty:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"JOHANSEN COINTEGRATION TEST RESULTS\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Pair':<15} {'Trace Stat':<12} {'Max Eigen':<12} {'Trace Signif':<15} {'Max Eigen Signif':<15} {'Data Points':<12}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"{row['Pair']:<15} {row['Trace_Statistic']:<12.4f} {row['Max_Eigen_Statistic']:<12.4f} {row['Trace_Significance']:<15} {row['Max_Eigen_Significance']:<15} {row['Data_Points']:<12}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DETAILED RESULTS\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Display detailed results for cointegrated pairs\n",
    "    cointegrated_pairs = results_df[\n",
    "        (results_df['Trace_Significance'] != 'Not Significant') | \n",
    "        (results_df['Max_Eigen_Significance'] != 'Not Significant')\n",
    "    ]\n",
    "    \n",
    "    if not cointegrated_pairs.empty:\n",
    "        print(\"\\nCOINTEGRATED PAIRS:\")\n",
    "        for _, row in cointegrated_pairs.iterrows():\n",
    "            print(f\"\\n{row['Pair']}:\")\n",
    "            print(f\"  Trace Statistic: {row['Trace_Statistic']:.4f}\")\n",
    "            print(f\"  Trace Critical Values: 90%={row['Trace_90%_Critical']:.4f}, 95%={row['Trace_95%_Critical']:.4f}, 99%={row['Trace_99%_Critical']:.4f}\")\n",
    "            print(f\"  Trace Significance: {row['Trace_Significance']}\")\n",
    "            print(f\"  Max Eigenvalue Statistic: {row['Max_Eigen_Statistic']:.4f}\")\n",
    "            print(f\"  Max Eigen Critical Values: 90%={row['Max_Eigen_90%_Critical']:.4f}, 95%={row['Max_Eigen_95%_Critical']:.4f}, 99%={row['Max_Eigen_99%_Critical']:.4f}\")\n",
    "            print(f\"  Max Eigen Significance: {row['Max_Eigen_Significance']}\")\n",
    "            print(f\"  Data Points: {row['Data_Points']}\")\n",
    "    else:\n",
    "        print(\"\\nNo significantly cointegrated pairs found at 90% confidence level or higher.\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv('johansen_cointegration_results.csv', index=False)\n",
    "    print(f\"\\nResults saved to 'johansen_cointegration_results.csv'\")\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

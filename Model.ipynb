{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b249154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import sklearn as sk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from hmmlearn import hmm , vhmm\n",
    "import scipy.fftpack as fftpack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from ta.trend import sma_indicator\n",
    "from ta.momentum import rsi\n",
    "from ta.momentum import stoch\n",
    "from ta.trend import ema_indicator\n",
    "from ta.trend import adx\n",
    "from ta.trend import macd\n",
    "from ta.volume import on_balance_volume\n",
    "import random\n",
    "import itertools\n",
    "from collections import deque\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a047ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity:int):\n",
    "        self.capacity = capacity\n",
    "        self.memory  = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, s, a, r, s_next, done):\n",
    "        self.memory.append((s, a, r, s_next, done))\n",
    "\n",
    "    def sample(self, batch_size:int):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        s,a,r,sn,d = map(np.stack, zip(*batch))\n",
    "        return (torch.tensor(s,  dtype=torch.float32, device=device),\n",
    "                torch.tensor(a,  dtype=torch.int64,   device=device),\n",
    "                torch.tensor(r,  dtype=torch.float32, device=device),\n",
    "                torch.tensor(sn, dtype=torch.float32, device=device),\n",
    "                torch.tensor(d,  dtype=torch.bool,    device=device))\n",
    "\n",
    "    def __len__(self): return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target_net: nn.Module,\n",
    "                online_net: nn.Module,\n",
    "                tau: float = 0.01):\n",
    "   \n",
    "    for t_param, o_param in zip(target_net.parameters(),  online_net.parameters()):\n",
    "        t_param.data.mul_(1.0 - tau).add_(tau * o_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPolicyNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=3):\n",
    "        super(LSTMPolicyNetwork, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layers (batch_first keeps (batch, seq, feat) ordering)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "\n",
    "        # Output head â†’ action probabilities\n",
    "        self.action_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "      \n",
    "        if hidden is None:\n",
    "            hidden = self._init_hidden(x.size(0), x.device)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # Use the output from the final time step\n",
    "        action_probs = self.action_head(lstm_out[:, -1, :])\n",
    "\n",
    "        return action_probs, hidden\n",
    "\n",
    "    def _init_hidden(self, batch_size, device):\n",
    "        # (h_0, c_0) both zero-initialised\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        return (h0, c0)\n",
    "\n",
    "    def select_action(self, state, hidden=None, test_mode=False):\n",
    "        # Ensure tensor\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.FloatTensor(state)\n",
    "\n",
    "        state = state.to(device)\n",
    "\n",
    "        # Add batch dim if absent\n",
    "        if state.dim() == 2:         # [seq_len, features]\n",
    "            state = state.unsqueeze(0)  # -> [1, seq_len, features]\n",
    "        elif state.dim() > 3:\n",
    "            raise ValueError(f\"Input state has {state.dim()} dimensions \"\n",
    "                             f\"but should have 2 or 3 (got {state.shape})\")\n",
    "\n",
    "        action_probs, hidden = self(state, hidden)\n",
    "\n",
    "        if test_mode:\n",
    "            action = torch.argmax(action_probs, dim=1).item()\n",
    "            return action, hidden, action_probs\n",
    "\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        return action.item(), hidden, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMValueNetwork(nn.Module):\n",
    "  \n",
    "    def __init__(self, input_size, hidden_size: int = 64, num_layers: int = 3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor,\n",
    "                hidden: tuple[torch.Tensor, torch.Tensor] | None = None):\n",
    "        if hidden is None:\n",
    "            h_0 = torch.zeros(\n",
    "                self.num_layers, x.size(0), self.hidden_size, device=x.device\n",
    "            )\n",
    "            c_0 = torch.zeros_like(h_0)\n",
    "            hidden = (h_0, c_0)\n",
    "\n",
    "        lstm_out, hidden = self.lstm(x, hidden)        # lstm_out: (B, T, H)\n",
    "        state_value = self.value_head(lstm_out[:, -1, :]).squeeze(-1)\n",
    "\n",
    "        return state_value, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad453e3",
   "metadata": {},
   "source": [
    "# Trader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientTrader:\n",
    "\n",
    "    def __init__(self, features, sequence_length=5, hidden_size=32, num_layers=1,\n",
    "                 transaction_cost=0.001,gamma=0.99,entropy_coef=0.000,batch_size=128,update_cycle=25,replay_capacity =1000,tau=1,update_nets=1):\n",
    "        self.device = device\n",
    "        self.features = features\n",
    "        self.available_features = features \n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_scaler = MinMaxScaler()\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.action_map = {0: 0, 1: -1, 2: 1}  # Neutral, Short/long, Long/short      \n",
    "        self.input_size = None\n",
    "        self.policy = None\n",
    "        self.optimizer = None\n",
    "        self.value_net        = None  \n",
    "        self.value_optimizer  = None  \n",
    "        self.value_loss_fn    = nn.MSELoss()\n",
    "        self.gamma=gamma\n",
    "        self.entropy_coef = entropy_coef \n",
    "        self.policy_lr = 1e-4\n",
    "        self.value_lr = 1e-3\n",
    "        self.update_net =update_nets      \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.replay_capacity = replay_capacity\n",
    "        self.batch_size      = batch_size  \n",
    "        self.update_cycle    = update_cycle     \n",
    "        self.buffer   = ReplayBuffer(self.replay_capacity)\n",
    "        self.target_value_net = None    \n",
    "        self.tau             = tau  \n",
    "        self.step_counter    = 0\n",
    "\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "\n",
    "        # Check if all features exist in the DataFrame\n",
    "        available_features = []\n",
    "        for feature in self.features:\n",
    "            if feature in data.columns:\n",
    "                available_features.append(feature)\n",
    "            else:\n",
    "                print(f\"Warning: Feature '{feature}' not found in data. Skipping.\")\n",
    "\n",
    "        if not available_features:\n",
    "            raise ValueError(f\"None of the specified features {self.features} found in the data columns: {data.columns}\")\n",
    "\n",
    "        # Extract relevant features and scale them\n",
    "        X = data[available_features].values\n",
    "\n",
    "        # Store the available features for later use\n",
    "        self.available_features = available_features\n",
    "\n",
    "    \n",
    "        if not hasattr(self, 'feature_scaler_fitted') or not self.feature_scaler_fitted:\n",
    "            self.feature_scaler.fit(X)\n",
    "            self.feature_scaler_fitted = True\n",
    "\n",
    "        # Scale features\n",
    "        X_scaled = self.feature_scaler.transform(X)\n",
    "\n",
    "        return X_scaled\n",
    "\n",
    "    def calculate_cumulative_return(self, returns):\n",
    "     \n",
    "        return np.sum(returns)\n",
    "\n",
    "    def calculate_sharpe_ratio(self, returns, risk_free_rate=0.0):\n",
    "  \n",
    "        if len(returns) < 2:\n",
    "            return 0\n",
    "\n",
    "        mean_return = np.mean(returns) * 252  # Annualized\n",
    "        std_return = np.std(returns) * np.sqrt(252)  # Annualized\n",
    "\n",
    "        if std_return == 0:\n",
    "            return 0\n",
    "\n",
    "        return (mean_return - risk_free_rate) / std_return\n",
    "\n",
    "    def _step_env(self, data, t, action_idx):\n",
    "   \n",
    "        new_position = self.action_map[action_idx]\n",
    "\n",
    "        price_ret1  = data['Return1'].values[self.sequence_length + t]\n",
    "        price_ret2  = data['Return2'].values[self.sequence_length + t]\n",
    "        hedge_ratio = 1 #data['Hedge Ratio'].values[self.sequence_length + t - 1]\n",
    "        tc          = self.transaction_cost* abs(new_position - self.current_position)\n",
    "\n",
    "        if   new_position ==  1: r_t =  price_ret1 - hedge_ratio*price_ret2 - tc\n",
    "        elif new_position == -1: r_t = -price_ret1 + hedge_ratio*price_ret2 - tc\n",
    "        else:                     r_t = -tc                     # flat\n",
    "\n",
    "        return r_t, new_position\n",
    "\n",
    "    def train_online(self, data):\n",
    "        X = self.preprocess_data(data)\n",
    "        n_steps = len(X) - self.sequence_length\n",
    "\n",
    "    \n",
    "        # Initialize policy network if not done yet\n",
    "        if self.policy is None:\n",
    "            # Number of features plus position\n",
    "            self.input_size = X.shape[1] + 1\n",
    "            self.policy = LSTMPolicyNetwork(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                num_layers=self.num_layers\n",
    "            ).to(self.device)\n",
    "            self.optimizer = optim.Adam(self.policy.parameters(), lr=self.policy_lr)\n",
    "            \n",
    "            self.value_net = LSTMValueNetwork(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                num_layers=self.num_layers\n",
    "            ).to(self.device)\n",
    "            self.value_optimizer = optim.Adam(self.value_net.parameters(),    lr=self.value_lr)\n",
    "            print(f\"Initialized policy network with input size {self.input_size} (includes position feature)\")\n",
    "    \n",
    "            self.target_value_net = copy.deepcopy(self.value_net).eval()\n",
    "        positions = [0]          # start flat; length will be len(returns)+1\n",
    "        returns   = []           # one reward per market step\n",
    "        hidden = None\n",
    "        self.current_position = 0\n",
    "        for t in range(n_steps):\n",
    "\n",
    "            # build current state s_t\n",
    "            seq       = X[t : t+self.sequence_length]\n",
    "            pos_feat  = np.full((self.sequence_length,1), self.current_position)\n",
    "            s_t_np    = np.hstack((seq, pos_feat))\n",
    "            s_t       = torch.tensor(s_t_np, dtype=torch.float32, device=self.device)\n",
    "\n",
    "            # actor chooses action \n",
    "            action_idx, hidden, log_p = self.policy.select_action(s_t, hidden)\n",
    "            hidden  = None\n",
    "            #hidden.detach()\n",
    "            a_t     = torch.tensor([action_idx], device=self.device)\n",
    "\n",
    "            # environment step \n",
    "            r_t, next_position = self._step_env(data, t, action_idx)\n",
    "\n",
    "            positions.append(next_position)   # track position for plotting\n",
    "            returns .append(r_t)             # track realised reward\n",
    "            self.current_position = next_position\n",
    "          \n",
    "\n",
    "            # build s_{t+1}\n",
    "            if t < n_steps-1:\n",
    "                seq_next    = X[t+1 : t+1+self.sequence_length]\n",
    "                pos_feat_n  = np.full((self.sequence_length,1), self.current_position)\n",
    "                s_next_np   = np.hstack((seq_next, pos_feat_n))\n",
    "                s_next      = torch.tensor(s_next_np, dtype=torch.float32, device=self.device)\n",
    "                done        = False\n",
    "            else:          # last sample\n",
    "                s_next, done = torch.zeros_like(s_t), True\n",
    "\n",
    "            # store transition\n",
    "            self.buffer.push(s_t_np, action_idx, r_t, s_next.cpu().numpy(), done)\n",
    "\n",
    "            # every update_cycle steps: gradient step\n",
    "            if len(self.buffer) >= self.batch_size:\n",
    "                if (t+1)%self.update_net==0:\n",
    "                    self._learn_from_buffer()\n",
    "                if (t+1) % self.update_cycle == 0:\n",
    "                    #  update target critic\n",
    "                    soft_update(self.target_value_net, self.value_net, self.tau)\n",
    "        results = {\n",
    "            'total_return': self.calculate_cumulative_return(np.array(returns)),\n",
    "            'sharpe_ratio': self.calculate_sharpe_ratio     (np.array(returns)),\n",
    "            'positions'   : positions[1:],   # drop the initial neutral pos\n",
    "            'returns'     : returns,\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _learn_from_buffer(self):\n",
    "        S,A,R,SN,D = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        #  Critic\n",
    "        with torch.no_grad():\n",
    "            v_next, _ = self.target_value_net(SN)\n",
    "            td_target = R + self.gamma * v_next * (~D)\n",
    "\n",
    "        v, _      = self.value_net(S)\n",
    "        td_error  = td_target - v\n",
    "\n",
    "        critic_loss = td_error.pow(2).mean()            \n",
    "\n",
    "        self.value_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.value_net.parameters(), 1.0)\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        #  Actor  \n",
    "        action_probs, _ = self.policy(S)\n",
    "        dist       = Categorical(action_probs)\n",
    "        log_prob_a = dist.log_prob(A)\n",
    "\n",
    "        actor_loss = -(log_prob_a * td_error.detach()).mean()  \n",
    "        entropy    = dist.entropy().mean()\n",
    "        total_loss = actor_loss - self.entropy_coef * entropy   \n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, path):\n",
    "    \n",
    "        torch.save({\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'feature_scaler': self.feature_scaler,\n",
    "            'features': self.features,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'feature_scaler_fitted': self.feature_scaler_fitted,\n",
    "            'input_size': self.input_size,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'num_layers': self.num_layers\n",
    "        }, path)\n",
    "\n",
    "    def load_model(self, path, device=None):\n",
    "     \n",
    "        # Set device if specified\n",
    "        if device is not None:\n",
    "            self.device = self._get_device(device)\n",
    "            print(f\"Using device: {self.device}\")\n",
    "\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "\n",
    "       \n",
    "        self.features = checkpoint['features']\n",
    "        self.sequence_length = checkpoint['sequence_length']\n",
    "        self.feature_scaler = checkpoint['feature_scaler']\n",
    "        self.feature_scaler_fitted = checkpoint['feature_scaler_fitted']\n",
    "        self.input_size = checkpoint['input_size']\n",
    "        self.hidden_size = checkpoint.get('hidden_size', self.hidden_size)\n",
    "        self.num_layers = checkpoint.get('num_layers', self.num_layers)\n",
    "\n",
    "  \n",
    "        self.policy = LSTMPolicyNetwork(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Load the state dict\n",
    "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.learning_rate)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c6145",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = #pd.read_csv(r\"DG DLTR.csv\")\n",
    "# train_split=1\n",
    "# val_split=0.1\n",
    "# total_size = len(data)\n",
    "# test_size = int(total_size * (1 - train_split))\n",
    "# train_val_size = total_size - test_size\n",
    "# val_size = int(train_val_size *val_split)\n",
    "# train_size = train_val_size - val_size\n",
    " \n",
    "# train_data = data.iloc[:train_size]\n",
    "# val_data = data.iloc[train_size:train_size + val_size]\n",
    "# test_data = data.iloc[train_val_size:]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427e557",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6fc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, random, numpy as np, pandas as pd, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tune_pg_trader(\n",
    "        data: pd.DataFrame,\n",
    "        param_grid: dict,\n",
    "        n_runs: int = 5,\n",
    "        hidden_size: int = 32,\n",
    "        seed: int = 42,\n",
    "        device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        verbose: bool = True,\n",
    "):\n",
    "    def _make_hashable(x):\n",
    "       \n",
    "        return tuple(x) if isinstance(x, list) else x\n",
    "  \n",
    "    # build grid \n",
    "    param_names = list(param_grid.keys())\n",
    "    grid = list(itertools.product(*(param_grid[p] for p in param_names)))\n",
    "\n",
    "    metrics_records = []\n",
    "\n",
    "    outer_iter = tqdm(grid, desc=\"Param combos\", leave=False) if verbose else grid\n",
    "    for combo in outer_iter:\n",
    "        params = dict(zip(param_names, combo))\n",
    "\n",
    "        inner_iter = range(n_runs)\n",
    "        if verbose:\n",
    "            inner_iter = tqdm(inner_iter, desc=f\"Runs {params}\", leave=False)\n",
    "\n",
    "        for run_idx in inner_iter:\n",
    "            # deterministic but unique seeds per run \n",
    "            this_seed = seed + run_idx\n",
    "            torch.manual_seed(this_seed)\n",
    "            np.random.seed(this_seed)\n",
    "            random.seed(this_seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(this_seed)\n",
    "\n",
    "            # uild trader\n",
    "            trader = PolicyGradientTrader(\n",
    "                features=params[\"features\"],\n",
    "                sequence_length=params[\"sequence_length\"],\n",
    "                gamma=params[\"gamma\"],\n",
    "                entropy_coef=params[\"entropy_coef\"],\n",
    "                batch_size=params[\"batch_size\"],\n",
    "                update_cycle=params[\"update_cycle\"],\n",
    "                replay_capacity = params['replay_capacity'],\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=params['num_layers'],\n",
    "            )\n",
    "            safe_params = {k: _make_hashable(v) for k, v in params.items()}\n",
    "\n",
    "            res = trader.train_online(data)          # online pass\n",
    "            run_returns = np.array(res[\"returns\"])\n",
    "            mean_ret     = np.mean(run_returns)\n",
    "            sharpe      = trader.calculate_sharpe_ratio(run_returns)\n",
    "            vol         = np.std(run_returns) * np.sqrt(252)\n",
    "\n",
    "            metrics_records.append({\n",
    "                **safe_params,\n",
    "                \"run\": run_idx,\n",
    "                \"mean_return\": mean_ret,\n",
    "                \"sharpe\": sharpe,\n",
    "                \"volatility\": vol\n",
    "            })\n",
    "            #  cleanup GPU\n",
    "            del trader, res, run_returns       \n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    #  assemble results \n",
    "    metrics_df = pd.DataFrame(metrics_records)\n",
    "\n",
    "    # mean metrics per hyper-parameter combination\n",
    "    agg_cols = [\"mean_return\", \"sharpe\", \"volatility\"]\n",
    "    summary_df = (\n",
    "        metrics_df\n",
    "        .groupby(param_names, as_index=False)[agg_cols]\n",
    "        .mean()\n",
    "        .rename(columns={c: f\"mean_{c}\" for c in agg_cols})\n",
    "    )\n",
    "\n",
    "    # pick best by mean cumulative return\n",
    "    best_idx = summary_df[\"mean_mean_return\"].idxmax()\n",
    "    summary_df[\"is_best\"] = False\n",
    "    summary_df.loc[best_idx, \"is_best\"] = True\n",
    "\n",
    "    return metrics_df, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10628386",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_tune = {\n",
    "    \"sequence_length\": [5,10,15,20,30],\n",
    "    \"gamma\":          [0.99,0.95],\n",
    "    \"entropy_coef\":   [0.0],\n",
    "    \"batch_size\":     [64,128,256],\n",
    "    \"update_cycle\":   [ 10,15,20,25],\n",
    "    \"replay_capacity\":[1000,2000],\n",
    "    \"features\": [\n",
    "        ['Spread','Spread Long'],\n",
    "        ['Spread','rsi2','macd2','rsi1','macd1','obv1','obv2'],\n",
    "        ['Spread Long','rsi2','macd2','rsi1','macd1','obv1','obv2'],\n",
    "        ['Spread'],\n",
    "        ['Spread Long']\n",
    "\n",
    "    ],\n",
    "    'transaction_cost':[0.0005],\n",
    "    'num_layers':[1,2],\n",
    "    'update_net':[1]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "metrics_df, summary_df = tune_pg_trader(\n",
    "    data=data,            # your validation DataFrame\n",
    "    param_grid=param_grid_tune,\n",
    "    n_runs=4\n",
    ")\n",
    "\n",
    "print(summary_df.sort_values(\"mean_mean_return\", ascending=False))\n",
    "print(metrics_df.sort_values(\"mean_return\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc49962",
   "metadata": {},
   "source": [
    "# Plotting and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97377929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as scipy_stats\n",
    "def plot_results(\n",
    "        data,\n",
    "        positions_list,\n",
    "        returns_list,\n",
    "        *,\n",
    "        timescale: str = \"daily\",       \n",
    "        random_seed: int = 42,\n",
    "        sequence_length: int = 10,\n",
    "        transaction_cost: float = 0.0005\n",
    "    ):\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Annualisation factor\n",
    "    ts = timescale.lower()\n",
    "    if ts == \"daily\":\n",
    "        periods_per_year = 252\n",
    "    elif ts == \"hourly\":\n",
    "        periods_per_year = 252 * 6      # 6 trading hours per day\n",
    "    else:\n",
    "        raise ValueError(\"`timescale` must be 'daily' or 'hourly'\")\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Input validation\n",
    "    if len(positions_list) != len(returns_list):\n",
    "        raise ValueError(\"positions_list and returns_list must have the same length\")\n",
    "    n_runs = len(returns_list)\n",
    "    if n_runs == 0:\n",
    "        raise ValueError(\"No runs provided\")\n",
    "\n",
    "    print(f\"Analyzing {n_runs} runs ({timescale}) â€¦\")\n",
    "\n",
    "    # Buy-and-Hold benchmarks\n",
    "    ret1 = data[\"Return1\"].values[sequence_length:]\n",
    "    ret2 = data[\"Return2\"].values[sequence_length:]\n",
    "    bh1_cum = np.exp(ret1.cumsum()) - 1.0\n",
    "    bh2_cum = np.exp(ret2.cumsum()) - 1.0\n",
    "\n",
    "    #  Fixed-threshold strategies \n",
    "    spread = data[\"Spread\"].values\n",
    "    pr1 = data[\"Return1\"].values\n",
    "    pr2 = data[\"Return2\"].values\n",
    "\n",
    "    def _fixed_threshold(thr: float):\n",
    "        \n",
    "        pos = np.where(spread > thr, 1, np.where(spread < -thr, -1, 0))\n",
    "        pnl = pos[:-1] * (pr1[1:] - pr2[1:])               # hedge_ratioâ‰ˆ1\n",
    "        switch = np.abs(np.diff(np.insert(pos, 0, 0)))\n",
    "        tc = transaction_cost * switch[:-1]\n",
    "        return pnl - tc\n",
    "\n",
    "    returns_05 = _fixed_threshold(0.5)[: len(returns_list[0])]\n",
    "    returns_1  = _fixed_threshold(1.0)[: len(returns_list[0])]\n",
    "\n",
    "    #  Random strategies \n",
    "    def _random_strategy(length: int, seed_offset: int = 0):\n",
    "        np.random.seed(random_seed + seed_offset)\n",
    "        rand_pos = np.random.choice([-1, 0, 1], size=length)\n",
    "        cur_pos = 0\n",
    "        rand_ret = []\n",
    "        for i, new_pos in enumerate(rand_pos):\n",
    "           \n",
    "            if new_pos == 1:\n",
    "                r = pr1[sequence_length + i] - pr2[sequence_length + i]\n",
    "            elif new_pos == -1:\n",
    "                r = -pr1[sequence_length + i] + pr2[sequence_length + i]\n",
    "            else:\n",
    "                r = 0\n",
    "            tc = transaction_cost * abs(new_pos - cur_pos)\n",
    "            rand_ret.append(r - tc)\n",
    "            cur_pos = new_pos\n",
    "        return np.asarray(rand_ret), rand_pos\n",
    "\n",
    "    n_random_runs = max(10, n_runs)\n",
    "    random_returns_list, random_positions_list = [], []\n",
    "    for i in range(n_random_runs):\n",
    "        rr, rp = _random_strategy(len(returns_list[0]), i)\n",
    "        random_returns_list.append(rr)\n",
    "        random_positions_list.append(rp)\n",
    "\n",
    "    #  per-run statistics \n",
    "    def _stats(r: np.ndarray):\n",
    "    \n",
    "        if r.size == 0:\n",
    "            return 0, 0, 0, 0, 0, 0\n",
    "        step_mean = r.mean()\n",
    "        step_std  = r.std()\n",
    "        total     = r.sum()\n",
    "        ann_mean  = step_mean * periods_per_year\n",
    "        ann_vol   = step_std  * np.sqrt(periods_per_year)\n",
    "        sharpe    = ann_mean / ann_vol if ann_vol else 0\n",
    "        return total, ann_vol, ann_mean, sharpe, step_mean, step_std\n",
    "\n",
    "    # RL statistics\n",
    "    rl_stats   = np.array([_stats(r) for r in returns_list])          # (n_runs, 6)\n",
    "    rl_cum     = [np.exp(r.cumsum()) - 1.0 for r in returns_list]\n",
    "\n",
    "    # Random statistics\n",
    "    rand_stats = np.array([_stats(r) for r in random_returns_list])\n",
    "    rand_cum   = [np.exp(r.cumsum()) - 1.0 for r in random_returns_list]\n",
    "\n",
    "    # Benchmarks\n",
    "    bh1_stats    = _stats(ret1)\n",
    "    bh2_stats    = _stats(ret2)\n",
    "    fix05_stats  = _stats(returns_05)\n",
    "    fix1_stats   = _stats(returns_1)\n",
    "\n",
    "    # Aggregate statistics\n",
    "    rl_mean,  rl_std  = rl_stats.mean(0),  rl_stats.std(0)\n",
    "    rand_mean, rand_std = rand_stats.mean(0), rand_stats.std(0)\n",
    "\n",
    "    stats_df = pd.DataFrame({\n",
    "        \"Strategy\": [\n",
    "            \"RL Agent (Î¼)\", \"RL Agent (Ïƒ)\",\n",
    "            \"Random (Î¼)\",   \"Random (Ïƒ)\",\n",
    "            \"Buy&Hold 1\",   \"Buy&Hold 2\",\n",
    "            \"Fixed 0.5\",    \"Fixed 1.0\"],\n",
    "        \"Total Return\": [\n",
    "            f\"{rl_mean[0]:.4f}\", f\"Â±{rl_std[0]:.4f}\",\n",
    "            f\"{rand_mean[0]:.4f}\", f\"Â±{rand_std[0]:.4f}\",\n",
    "            f\"{bh1_stats[0]:.4f}\", f\"{bh2_stats[0]:.4f}\",\n",
    "            f\"{fix05_stats[0]:.4f}\", f\"{fix1_stats[0]:.4f}\"],\n",
    "        \"Ann Return\": [\n",
    "            f\"{rl_mean[2]:.4f}\", f\"Â±{rl_std[2]:.4f}\",\n",
    "            f\"{rand_mean[2]:.4f}\", f\"Â±{rand_std[2]:.4f}\",\n",
    "            f\"{bh1_stats[2]:.4f}\", f\"{bh2_stats[2]:.4f}\",\n",
    "            f\"{fix05_stats[2]:.4f}\", f\"{fix1_stats[2]:.4f}\"],\n",
    "        \"Volatility\": [\n",
    "            f\"{rl_mean[1]:.4f}\", f\"Â±{rl_std[1]:.4f}\",\n",
    "            f\"{rand_mean[1]:.4f}\", f\"Â±{rand_std[1]:.4f}\",\n",
    "            f\"{bh1_stats[1]:.4f}\", f\"{bh2_stats[1]:.4f}\",\n",
    "            f\"{fix05_stats[1]:.4f}\", f\"{fix1_stats[1]:.4f}\"],\n",
    "        \"Sharpe\": [\n",
    "            f\"{rl_mean[3]:.4f}\", f\"Â±{rl_std[3]:.4f}\",\n",
    "            f\"{rand_mean[3]:.4f}\", f\"Â±{rand_std[3]:.4f}\",\n",
    "            f\"{bh1_stats[3]:.4f}\", f\"{bh2_stats[3]:.4f}\",\n",
    "            f\"{fix05_stats[3]:.4f}\", f\"{fix1_stats[3]:.4f}\"],\n",
    "        \"Step Mean\": [\n",
    "            f\"{rl_mean[4]:.6f}\", f\"Â±{rl_std[4]:.6f}\",\n",
    "            f\"{rand_mean[4]:.6f}\", f\"Â±{rand_std[4]:.6f}\",\n",
    "            f\"{bh1_stats[4]:.6f}\", f\"{bh2_stats[4]:.6f}\",\n",
    "            f\"{fix05_stats[4]:.6f}\", f\"{fix1_stats[4]:.6f}\"],\n",
    "        \"Step Std\": [\n",
    "            f\"{rl_mean[5]:.6f}\", f\"Â±{rl_std[5]:.6f}\",\n",
    "            f\"{rand_mean[5]:.6f}\", f\"Â±{rand_std[5]:.6f}\",\n",
    "            f\"{bh1_stats[5]:.6f}\", f\"{bh2_stats[5]:.6f}\",\n",
    "            f\"{fix05_stats[5]:.6f}\", f\"{fix1_stats[5]:.6f}\"]\n",
    "    })\n",
    "\n",
    "    #plotting\n",
    "    matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "    # (a) Cumulative returns\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for c in rl_cum:\n",
    "        plt.plot(c * 100, color=\"blue\", alpha=0.25, linewidth=0.8)\n",
    "    rl_mean_cum = np.mean(rl_cum, 0)\n",
    "    rl_std_cum  = np.std(rl_cum, 0)\n",
    "    plt.plot(rl_mean_cum * 100, color=\"blue\", linewidth=2.5,\n",
    "             label=f\"RL Strategy (n={n_runs})\")\n",
    "    plt.fill_between(range(len(rl_mean_cum)),\n",
    "                     (rl_mean_cum - rl_std_cum) * 100,\n",
    "                     (rl_mean_cum + rl_std_cum) * 100, color=\"blue\",\n",
    "                     alpha=0.2)\n",
    "\n",
    "    rand_mean_cum = np.mean(rand_cum, 0)\n",
    "    rand_std_cum  = np.std(rand_cum, 0)\n",
    "    plt.plot(rand_mean_cum * 100, \"--\", color=\"gray\", linewidth=2,\n",
    "             label=f\"Random (n={n_random_runs})\")\n",
    "    plt.fill_between(range(len(rand_mean_cum)),\n",
    "                     (rand_mean_cum - rand_std_cum) * 100,\n",
    "                     (rand_mean_cum + rand_std_cum) * 100, color=\"gray\",\n",
    "                     alpha=0.15)\n",
    "\n",
    "    plt.plot(bh1_cum * 100, label=\"Buy & Hold 1\", color=\"green\", lw=1.5)\n",
    "    plt.plot(bh2_cum * 100, label=\"Buy & Hold 2\", color=\"orange\", lw=1.5)\n",
    "    plt.plot((np.exp(returns_05.cumsum()) - 1) * 100,\n",
    "             label=\"Fixed 0.5\", color=\"red\", lw=1.2)\n",
    "    plt.plot((np.exp(returns_1.cumsum()) - 1) * 100,\n",
    "             label=\"Fixed 1.0\", color=\"purple\", lw=1.2)\n",
    "    plt.ylabel(\"Cumulative Return (%)\")\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.title(\"Cumulative Performance Comparison\")\n",
    "    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # (b) Position distribution\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    pos_all = np.concatenate(positions_list)\n",
    "    counts = np.bincount(pos_all + 1)              # [-1,0,1] â†’ [0,1,2]\n",
    "    labels = [\"Short/Long (-1)\", \"Flat (0)\", \"Long/Short (+1)\"]\n",
    "    bars = plt.bar(labels, counts, color=[\"red\", \"gray\", \"green\"], alpha=0.7)\n",
    "    total = counts.sum()\n",
    "    for b, c in zip(bars, counts):\n",
    "        plt.text(b.get_x() + b.get_width()/2, b.get_height()*1.01,\n",
    "                 f\"({100*c/total:.1f}%)\", ha=\"center\", va=\"bottom\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Position Distribution Across {n_runs} Runs\")\n",
    "    plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # (c) Return-distribution comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    rl_flat   = np.concatenate(returns_list)\n",
    "    rand_flat = np.concatenate(random_returns_list)\n",
    "    plt.hist(rl_flat,  bins=50, density=True, alpha=0.7,\n",
    "             label=\"RL\",     color=\"blue\")\n",
    "    plt.hist(rand_flat, bins=50, density=True, alpha=0.7,\n",
    "             label=\"Random\", color=\"gray\")\n",
    "    plt.axvline(rl_flat.mean(),   color=\"blue\", ls=\"--\",\n",
    "                label=f\"RL Î¼={rl_flat.mean():.4e}\")\n",
    "    plt.axvline(rand_flat.mean(), color=\"gray\", ls=\"--\",\n",
    "                label=f\"Random Î¼={rand_flat.mean():.4e}\")\n",
    "    plt.xlabel(\"One-period return\"); plt.ylabel(\"Density\")\n",
    "    plt.title(\"Return Distribution Comparison\")\n",
    "    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # (d) Rolling Sharpe ratio\n",
    "    window = min(50, len(returns_list[0]) // 4)\n",
    "    def _roll_sharpe(r, w):\n",
    "        s   = pd.Series(r, dtype=float)\n",
    "        rm  = s.rolling(w).mean() * periods_per_year\n",
    "        rsd = s.rolling(w).std()  * np.sqrt(periods_per_year)\n",
    "        roll = (rm / rsd).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "        return roll.values\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    rl_roll = np.array([_roll_sharpe(r, window) for r in returns_list])\n",
    "    for s in rl_roll:\n",
    "        plt.plot(s, color=\"blue\", alpha=0.25, lw=0.8)\n",
    "    plt.plot(rl_roll.mean(0), color=\"blue\", lw=2.5, label=\"RL mean\")\n",
    "\n",
    "    rand_roll = np.array([_roll_sharpe(r, window)\n",
    "                          for r in random_returns_list[:n_runs]])\n",
    "    plt.plot(rand_roll.mean(0), \"--\", color=\"gray\", lw=2,\n",
    "             label=\"Random mean\")\n",
    "    plt.axhline(0, color=\"black\", alpha=0.3)\n",
    "    plt.ylabel(\"Rolling Sharpe\"); plt.xlabel(\"Time step\")\n",
    "    plt.title(f\"Rolling Sharpe Ratio (window = {window})\")\n",
    "    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # (e) Performance metrics bar-chart\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    metrics = [\"Total Return\", \"Sharpe\", \"Consistency\"]\n",
    "    # crude \"consistency\": 1 â€“ Ïƒ_tot/|Î¼_tot|\n",
    "    cons_rl   = 1 - rl_std[0]  / (abs(rl_mean[0])  + 1e-8)\n",
    "    cons_rand = 1 - rand_std[0]/ (abs(rand_mean[0]) + 1e-8)\n",
    "    mx_ret = max(rl_mean[0], rand_mean[0], bh1_stats[0], bh2_stats[0])\n",
    "    mx_sh  = max(rl_mean[3], rand_mean[3], bh1_stats[3], bh2_stats[3])\n",
    "    rl_norm   = [rl_mean[0]/mx_ret if mx_ret else 0,\n",
    "                 rl_mean[3]/mx_sh  if mx_sh  else 0,\n",
    "                 max(0, cons_rl)]\n",
    "    rand_norm = [rand_mean[0]/mx_ret if mx_ret else 0,\n",
    "                 rand_mean[3]/mx_sh  if mx_sh  else 0,\n",
    "                 max(0, cons_rand)]\n",
    "    x = np.arange(len(metrics)); w = 0.35\n",
    "    plt.bar(x - w/2, rl_norm,   w, label=\"RL\",     color=\"blue\",  alpha=0.7)\n",
    "    plt.bar(x + w/2, rand_norm, w, label=\"Random\", color=\"gray\",  alpha=0.7)\n",
    "    plt.xticks(x, metrics); plt.ylabel(\"Normalized Score\")\n",
    "    plt.title(\"Performance Metrics Comparison\")\n",
    "    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    #  Kolmogorov-Smirnov test \n",
    "    ks_stat, ks_p = scipy_stats.ks_2samp(rl_flat, rand_flat, alternative=\"two-sided\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPREHENSIVE PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(stats_df.to_string(index=False))\n",
    "    print(\"\\nADDITIONAL STATISTICS:\")\n",
    "    print(f\"Timescale:          {timescale}  (periods/year = {periods_per_year})\")\n",
    "    print(f\"RL runs:            {n_runs}\")\n",
    "    print(f\"Random runs:        {n_random_runs}\")\n",
    "    print(f\"Series length:      {len(returns_list[0])}\")\n",
    "    print(f\"Transaction cost:   {transaction_cost:.4f}\")\n",
    "    print(\"\\nKolmogorov-Smirnov test (RL vs Random):\")\n",
    "    print(f\"KS statistic = {ks_stat:.4f}\")\n",
    "    print(f\"P-value      = {ks_p:.4f}\")\n",
    "    print(f\"Different distributions? {'Yes' if ks_p < 0.05 else 'No'} (Î± = 0.05)\")\n",
    "\n",
    "    # Return results dict \n",
    "    return {\n",
    "        \"stats_df\": stats_df,\n",
    "        \"rl_stats\":   {\"mean\": rl_mean,   \"std\": rl_std,   \"runs\": rl_stats},\n",
    "        \"rand_stats\": {\"mean\": rand_mean, \"std\": rand_std, \"runs\": rand_stats},\n",
    "        \"benchmarks\": {\n",
    "            \"buy_hold_1\": bh1_stats, \"buy_hold_2\": bh2_stats,\n",
    "            \"fixed_0.5\":  fix05_stats, \"fixed_1.0\":  fix1_stats},\n",
    "        \"ks_test\": {\n",
    "            \"ks_statistic\": ks_stat,\n",
    "            \"p_value\": ks_p,\n",
    "            \"is_significant\": ks_p < 0.05},\n",
    "        \"n_runs\": n_runs,\n",
    "        \"n_random_runs\": n_random_runs,\n",
    "        \"periods_per_year\": periods_per_year,\n",
    "        \"timescale\": timescale\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "def run_multiple_experiments( data,  n_runs=10, verbose=True,  reset_model=True, save_models=False, model_save_path=None):\n",
    "\n",
    "    # Generate random seeds\n",
    "    np.random.seed(42)  # For reproducible seed generation\n",
    "    seeds_used = np.random.randint(0, 100000, n_runs).tolist()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Starting {n_runs} RL strategy runs...\")\n",
    "        print(f\"Seeds: {seeds_used}\")\n",
    "        if reset_model:\n",
    "            print(\"Model will be reset between runs\")\n",
    "        else:\n",
    "            print(\"Model will continue training across runs\")\n",
    "    \n",
    "    # Storage for results\n",
    "    positions_list = []\n",
    "    returns_list = []\n",
    "    results_list = []\n",
    "    run_statistics = []\n",
    "    \n",
    "   \n",
    "    # Progress bar setup\n",
    "    pbar = tqdm(range(n_runs), desc=\"RL Runs\") if verbose else range(n_runs)\n",
    "    \n",
    "    for run_idx in pbar:\n",
    "        seed = seeds_used[run_idx] \n",
    "        \n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "           \n",
    "        features = ['Spread', 'Spread Long']\n",
    "        trader = PolicyGradientTrader(\n",
    "            features=features,\n",
    "            sequence_length=10,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            gamma=0.99,\n",
    "            update_cycle =20,\n",
    "            tau=1,\n",
    "            batch_size=64,\n",
    "            replay_capacity=1000,\n",
    "            transaction_cost = 0.0005\n",
    "        )\n",
    "        results = trader.train_online(data) \n",
    "        \n",
    "        # Store results \n",
    "        positions_list.append(np.array(results['positions']))\n",
    "        returns_list.append(np.array(results['returns']))\n",
    "        results_list.append(results)\n",
    "        \n",
    "        #  statistics \n",
    "        returns_array = np.array(results['returns'])\n",
    "        run_stats = {\n",
    "            'run_index': run_idx,\n",
    "            'seed': seed,\n",
    "            'total_return': results['total_return'],\n",
    "            'sharpe_ratio': results['sharpe_ratio'],\n",
    "            'n_trades': len(returns_array),\n",
    "            'win_rate': np.mean(returns_array > 0) if len(returns_array) > 0 else 0,\n",
    "            'volatility': np.std(returns_array) * np.sqrt(252) if len(returns_array) > 0 else 0,\n",
    "            'mean_return': np.mean(returns_array) if len(returns_array) > 0 else 0,\n",
    "            'final_position': results['positions'][-1] if len(results['positions']) > 0 else 0\n",
    "        }\n",
    "        run_statistics.append(run_stats)\n",
    "        \n",
    "       \n",
    "        if verbose and not isinstance(pbar, range):\n",
    "            pbar.set_postfix({\n",
    "                'Total_Ret': f\"{results['total_return']:.4f}\",\n",
    "                'Sharpe': f\"{results['sharpe_ratio']:.4f}\"\n",
    "            })\n",
    "                \n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nCompleted {n_runs} runs!\")\n",
    "        print(f\"Average Total Return: {np.mean([r['total_return'] for r in run_statistics]):.4f}\")\n",
    "        print(f\"Average Sharpe Ratio: {np.mean([r['sharpe_ratio'] for r in run_statistics]):.4f}\")\n",
    "        print(f\"Success Rate: {sum(1 for r in run_statistics if 'error' not in r)}/{n_runs}\")\n",
    "    \n",
    "    return {\n",
    "        'positions_list': positions_list,\n",
    "        'returns_list': returns_list,\n",
    "        'results_list': results_list,\n",
    "        'seeds_used': seeds_used,\n",
    "        'run_statistics': run_statistics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_results = run_multiple_experiments(\n",
    "    data=data, \n",
    "    n_runs=10, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "analysis = plot_results(\n",
    "    data=data,\n",
    "    positions_list=multi_results['positions_list'],\n",
    "    returns_list=multi_results['returns_list'],\n",
    "    timescale='hourly',\n",
    "    transaction_cost=0.0005\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
